// Copyright © Aptos Foundation
// Parts of the project are originally copyright © Meta Platforms, Inc.
// SPDX-License-Identifier: Apache-2.0

pub mod acquires_checker;
pub mod ast_simplifier;
mod bytecode_generator;
pub mod cyclic_instantiation_checker;
pub mod env_pipeline;
mod experiments;
mod file_format_generator;
pub mod flow_insensitive_checkers;
pub mod function_checker;
pub mod inliner;
pub mod logging;
pub mod options;
pub mod pipeline;
pub mod recursive_struct_checker;

use crate::{
    env_pipeline::{
        lambda_lifter, lambda_lifter::LambdaLiftingOptions, rewrite_target::RewritingScope,
        spec_checker, spec_rewriter, EnvProcessorPipeline,
    },
    pipeline::{
        ability_processor::AbilityProcessor, avail_copies_analysis::AvailCopiesAnalysisProcessor,
        copy_propagation::CopyPropagation, dead_store_elimination::DeadStoreElimination,
        exit_state_analysis::ExitStateAnalysisProcessor,
        livevar_analysis_processor::LiveVarAnalysisProcessor,
        reference_safety_processor::ReferenceSafetyProcessor,
        split_critical_edges_processor::SplitCriticalEdgesProcessor,
        uninitialized_use_checker::UninitializedUseChecker,
        unreachable_code_analysis::UnreachableCodeProcessor,
        unreachable_code_remover::UnreachableCodeRemover, variable_coalescing::VariableCoalescing,
    },
};
use anyhow::bail;
use codespan_reporting::term::termcolor::{ColorChoice, StandardStream, WriteColor};
pub use experiments::*;
use log::{debug, info, log_enabled, Level};
use move_binary_format::binary_views::BinaryIndexedView;
use move_command_line_common::files::FileHash;
use move_compiler::{
    compiled_unit::{
        verify_units, AnnotatedCompiledModule, AnnotatedCompiledScript, AnnotatedCompiledUnit,
        CompiledUnit, FunctionInfo, NamedCompiledModule, NamedCompiledScript,
    },
    diagnostics::FilesSourceText,
    shared::{known_attributes::KnownAttribute, unique_map::UniqueMap},
};
use move_disassembler::disassembler::Disassembler;
use move_ir_types::location;
use move_model::{
    add_move_lang_diagnostics,
    ast::{Address, ModuleName},
    model::GlobalEnv,
    PackageInfo,
};
use move_stackless_bytecode::function_target_pipeline::{
    FunctionTargetPipeline, FunctionTargetsHolder, FunctionVariant,
};
use move_symbol_pool::Symbol;
pub use options::*;
use std::{collections::BTreeSet, io::Write, path::Path};

/// Run Move compiler and print errors to stderr.
pub fn run_move_compiler_to_stderr(
    options: Options,
) -> anyhow::Result<(GlobalEnv, Vec<AnnotatedCompiledUnit>)> {
    let mut error_writer = StandardStream::stderr(ColorChoice::Auto);
    run_move_compiler(&mut error_writer, options)
}

/// Run move compiler and print errors to given writer. Returns the set of compiled units.
pub fn run_move_compiler<W>(
    error_writer: &mut W,
    options: Options,
) -> anyhow::Result<(GlobalEnv, Vec<AnnotatedCompiledUnit>)>
where
    W: WriteColor + Write,
{
    logging::setup_logging();
    info!("Move Compiler v2");

    // Run context check.
    let mut env = run_checker_and_rewriters(options.clone())?;
    check_errors(&env, error_writer, "checking errors")?;

    // Run code generator
    let mut targets = run_bytecode_gen(&env);
    check_errors(&env, error_writer, "code generation errors")?;
    debug!("After bytecode_gen, GlobalEnv={}", env.dump_env());

    // Run transformation pipeline
    let pipeline = bytecode_pipeline(&env);
    if log_enabled!(Level::Debug) {
        // Dump bytecode, providing a name for the target derived from the first input file.
        let dump_base_name = options
            .sources
            .first()
            .and_then(|f| {
                Path::new(f)
                    .file_name()
                    .map(|f| f.to_string_lossy().as_ref().to_owned())
            })
            .unwrap_or_else(|| "dump".to_owned());
        pipeline.run_with_dump(
            &env,
            &mut targets,
            &dump_base_name,
            false,
            &pipeline::register_formatters,
        )
    } else {
        pipeline.run(&env, &mut targets)
    }
    check_errors(&env, error_writer, "stackless-bytecode analysis errors")?;

    let modules_and_scripts = run_file_format_gen(&env, &targets);
    check_errors(&env, error_writer, "assembling errors")?;

    debug!(
        "File format bytecode:\n{}",
        disassemble_compiled_units(&modules_and_scripts)?
    );

    let annotated_units = annotate_units(modules_and_scripts);
    run_bytecode_verifier(&annotated_units, &mut env);
    check_errors(&env, error_writer, "bytecode verification errors")?;

    // Finally mark this model to be generated by v2
    env.set_compiler_v2(true);

    Ok((env, annotated_units))
}

/// Run move compiler and print errors to given writer for the purpose of analysis, like
/// e.g. the Move prover. After successful compilation attaches the generated bytecode
/// to the model.
pub fn run_move_compiler_for_analysis(
    error_writer: &mut impl WriteColor,
    mut options: Options,
) -> anyhow::Result<GlobalEnv> {
    options.whole_program = true; // will set `treat_everything_as_target`
    options = options.set_experiment(Experiment::SPEC_REWRITE, true);
    let (mut env, units) = run_move_compiler(error_writer, options)?;
    // Reset for subsequent analysis
    env.treat_everything_as_target(false);
    // Script pseudo module names are sequentially constructed as `<SELF>_1 .. <SELF>_n`. To
    // associate the bytecode module by name we need to count the index. This
    // assumes script modules come out in the same order as they are were
    // added to the environment.
    let mut script_index = 0; // script names are named using a sequential index
    for unit in units {
        let unit = unit.into_compiled_unit();
        match unit {
            CompiledUnit::Module(NamedCompiledModule {
                package_name: _,
                address,
                name,
                module,
                source_map,
            }) => {
                let name = ModuleName::new(
                    Address::Numerical(address.into_inner()),
                    env.symbol_pool().make(name.as_str()),
                );
                if let Some(id) = env.find_module(&name).map(|m| m.get_id()) {
                    env.attach_compiled_module(id, module, source_map)
                } else {
                    env.error(
                        &env.unknown_loc(),
                        &format!(
                            "failed to attach bytecode: cannot find module `{}`",
                            name.display_full(&env)
                        ),
                    );
                }
            },
            CompiledUnit::Script(NamedCompiledScript {
                package_name: _,
                name: _,
                script,
                source_map,
            }) => {
                let name = ModuleName::pseudo_script_name(env.symbol_pool(), script_index);
                script_index += 1;
                let module = move_model::script_into_module(
                    script,
                    &name.name().display(env.symbol_pool()).to_string(),
                );
                if let Some(id) = env.find_module(&name).map(|m| m.get_id()) {
                    env.attach_compiled_module(id, module, source_map)
                } else {
                    env.error(
                        &env.unknown_loc(),
                        &format!(
                            "failed to attach bytecode: cannot find script `{}`",
                            name.display_full(&env)
                        ),
                    );
                }
            },
        }
    }
    Ok(env)
}

/// Run the type checker and return the global env (with errors if encountered). The result
/// fails not on context checking errors, but possibly on i/o errors.
pub fn run_checker(options: Options) -> anyhow::Result<GlobalEnv> {
    info!("Type Checking");
    // Run the model builder, which performs context checking.
    let addrs = move_model::parse_addresses_from_options(options.named_address_mapping.clone())?;
    let mut env = move_model::run_model_builder_in_compiler_mode(
        PackageInfo {
            sources: options.sources.clone(),
            address_map: addrs.clone(),
        },
        vec![PackageInfo {
            sources: options.dependencies.clone(),
            address_map: addrs.clone(),
        }],
        options.skip_attribute_checks,
        if !options.skip_attribute_checks && options.known_attributes.is_empty() {
            KnownAttribute::get_all_attribute_names()
        } else {
            &options.known_attributes
        },
        !options.experiment_on(Experiment::ACQUIRES_CHECK),
    )?;
    // Store address aliases
    let map = addrs
        .into_iter()
        .map(|(s, a)| (env.symbol_pool().make(&s), a.into_inner()))
        .collect();
    env.set_address_alias_map(map);
    // Store options in env, for later access
    env.set_extension(options);
    Ok(env)
}

/// Run the type checker as well as the AST rewriting pipeline and related additional
/// checks, returning the global env (with errors if encountered). The result
/// fails not on context checking errors, but possibly on i/o errors.
pub fn run_checker_and_rewriters(options: Options) -> anyhow::Result<GlobalEnv> {
    let whole_program = options.whole_program;
    let scope = if whole_program {
        RewritingScope::Everything
    } else {
        RewritingScope::CompilationTarget
    };
    let env_pipeline = check_and_rewrite_pipeline(&options, false, scope);
    let mut env = run_checker(options)?;
    if !env.has_errors() {
        if whole_program {
            env.treat_everything_as_target(true)
        }
        env_pipeline.run(&mut env);
    }
    Ok(env)
}

// Run the (stackless) bytecode generator. For each function which is target of the
// compilation, create an entry in the functions target holder which encapsulate info
// like the generated bytecode.
pub fn run_bytecode_gen(env: &GlobalEnv) -> FunctionTargetsHolder {
    info!("Stackless bytecode Generation");
    let mut targets = FunctionTargetsHolder::default();
    let mut todo = BTreeSet::new();
    let mut done = BTreeSet::new();
    for module in env.get_modules() {
        if module.is_target() {
            for fun in module.get_functions() {
                let id = fun.get_qualified_id();
                todo.insert(id);
            }
        }
    }
    while let Some(id) = todo.pop_first() {
        done.insert(id);
        let func_env = env.get_function(id);
        let data = bytecode_generator::generate_bytecode(env, id);
        targets.insert_target_data(&id, FunctionVariant::Baseline, data);
        for callee in func_env
            .get_called_functions()
            .expect("called functions available")
        {
            if !done.contains(callee) {
                todo.insert(*callee);
            }
        }
    }
    targets
}

pub fn run_file_format_gen(env: &GlobalEnv, targets: &FunctionTargetsHolder) -> Vec<CompiledUnit> {
    info!("File Format Generation");
    file_format_generator::generate_file_format(env, targets)
}

/// Constructs the env checking and rewriting processing pipeline. `inlining_scope` can be set to
/// `Everything` for use with the Move Prover, otherwise `CompilationTarget`
/// should be used. If the model this is run on is produced via the v1 pipeline, the code
/// can be assumed already checked by the v1 compiler, so we skip some steps.
pub fn check_and_rewrite_pipeline<'a, 'b>(
    options: &'a Options,
    for_v1_model: bool,
    inlining_scope: RewritingScope,
) -> EnvProcessorPipeline<'b> {
    let mut env_pipeline = EnvProcessorPipeline::<'b>::default();

    if !for_v1_model && options.experiment_on(Experiment::USAGE_CHECK) {
        env_pipeline.add(
            "unused checks",
            flow_insensitive_checkers::check_for_unused_vars_and_params,
        );
        env_pipeline.add(
            "type parameter check",
            function_checker::check_for_function_typed_parameters,
        );
    }

    if !for_v1_model && options.experiment_on(Experiment::RECURSIVE_TYPE_CHECK) {
        env_pipeline.add("check recursive struct definition", |env| {
            recursive_struct_checker::check_recursive_struct(env)
        });
        env_pipeline.add("check cyclic type instantiation", |env| {
            cyclic_instantiation_checker::check_cyclic_instantiations(env)
        });
    }

    if !for_v1_model && options.experiment_on(Experiment::ACCESS_CHECK) {
        env_pipeline.add(
            "access and use check before inlining",
            |env: &mut GlobalEnv| function_checker::check_access_and_use(env, true),
        );
    }

    if options.experiment_on(Experiment::INLINING) {
        let keep_inline_funs = options.experiment_on(Experiment::KEEP_INLINE_FUNS);
        env_pipeline.add("inlining", {
            move |env| inliner::run_inlining(env, inlining_scope, keep_inline_funs)
        });
    }

    if !for_v1_model && options.experiment_on(Experiment::ACCESS_CHECK) {
        env_pipeline.add(
            "access and use check after inlining",
            |env: &mut GlobalEnv| function_checker::check_access_and_use(env, false),
        );
    }

    if !for_v1_model && options.experiment_on(Experiment::ACQUIRES_CHECK) {
        env_pipeline.add("acquires check", |env| {
            acquires_checker::acquires_checker(env)
        });
    }

    if options.experiment_on(Experiment::AST_SIMPLIFY_FULL) {
        env_pipeline.add("simplifier with code elimination", {
            move |env: &mut GlobalEnv| ast_simplifier::run_simplifier(env, true)
        });
    } else if options.experiment_on(Experiment::AST_SIMPLIFY) {
        env_pipeline.add("simplifier", {
            move |env: &mut GlobalEnv| ast_simplifier::run_simplifier(env, false)
        });
    }

    if options.experiment_on(Experiment::LAMBDA_LIFTING) {
        env_pipeline.add("lambda-lifting", |env: &mut GlobalEnv| {
            lambda_lifter::lift_lambdas(
                LambdaLiftingOptions {
                    include_inline_functions: true,
                },
                env,
            )
        });
    }

    if options.experiment_on(Experiment::SPEC_CHECK) {
        // Specification language checks are not done by the v1 compiler, so this
        // will always run.
        env_pipeline.add("specification checker", |env| {
            let env: &GlobalEnv = env;
            spec_checker::run_spec_checker(env)
        });
    }

    if options.experiment_on(Experiment::SPEC_REWRITE) {
        // Same as above for spec-check.
        env_pipeline.add("specification rewriter", spec_rewriter::run_spec_rewriter);
    }

    env_pipeline
}

/// Returns the bytecode processing pipeline.
pub fn bytecode_pipeline(env: &GlobalEnv) -> FunctionTargetPipeline {
    let options = env.get_extension::<Options>().expect("options");
    let mut pipeline = FunctionTargetPipeline::default();

    // --- Preprocessing of the stackless bytecode
    if options.experiment_on(Experiment::SPLIT_CRITICAL_EDGES) {
        pipeline.add_processor(Box::new(SplitCriticalEdgesProcessor {}));
    }

    // --- Checking correctness
    // These are various checks on bytecode level which do not modify
    // the code.
    if options.experiment_on(Experiment::UNINITIALIZED_CHECK) {
        let keep_annotations = options.experiment_on(Experiment::KEEP_UNINIT_ANNOTATIONS);
        pipeline.add_processor(Box::new(UninitializedUseChecker { keep_annotations }));
    }

    // Reference check is always run, but the processor decides internally
    // based on `Experiment::REFERENCE_SAFETY` whether to report errors.
    pipeline.add_processor(Box::new(LiveVarAnalysisProcessor::new(false)));
    pipeline.add_processor(Box::new(ReferenceSafetyProcessor {}));

    if options.experiment_on(Experiment::ABILITY_CHECK) {
        pipeline.add_processor(Box::new(ExitStateAnalysisProcessor {}));
        pipeline.add_processor(Box::new(AbilityProcessor {}));
    }

    // --- Optimizations
    // Any compiler errors or warnings should be reported before running this section, as we can
    // potentially delete or change code through these optimizations.
    // While this section of the pipeline is optional, some code that used to previously compile
    // may no longer compile without this section because of using too many local (temp) variables.

    if options.experiment_on(Experiment::DEAD_CODE_ELIMINATION) {
        pipeline.add_processor(Box::new(UnreachableCodeProcessor {}));
        pipeline.add_processor(Box::new(UnreachableCodeRemover {}));
        pipeline.add_processor(Box::new(LiveVarAnalysisProcessor::new(true)));
        pipeline.add_processor(Box::new(DeadStoreElimination {}));
    }

    if options.experiment_on(Experiment::VARIABLE_COALESCING) {
        // Live var analysis is needed by variable coalescing.
        pipeline.add_processor(Box::new(LiveVarAnalysisProcessor::new(false)));
        if options.experiment_on(Experiment::VARIABLE_COALESCING_ANNOTATE) {
            pipeline.add_processor(Box::new(VariableCoalescing::annotate_only()));
        }
        pipeline.add_processor(Box::new(VariableCoalescing::transform_only()));
    }

    if options.experiment_on(Experiment::COPY_PROPAGATION) {
        pipeline.add_processor(Box::new(AvailCopiesAnalysisProcessor {}));
        pipeline.add_processor(Box::new(CopyPropagation {}));
    }

    if options.experiment_on(Experiment::DEAD_CODE_ELIMINATION) {
        pipeline.add_processor(Box::new(LiveVarAnalysisProcessor::new(true)));
        pipeline.add_processor(Box::new(DeadStoreElimination {}));
    }

    // Run live var analysis again because it could be invalidated by previous pipeline steps,
    // but it is needed by file format generator.
    pipeline.add_processor(Box::new(LiveVarAnalysisProcessor::new(false)));
    pipeline
}

/// Disassemble the given compiled units and return the disassembled code as a string.
pub fn disassemble_compiled_units(units: &[CompiledUnit]) -> anyhow::Result<String> {
    let disassembled_units: anyhow::Result<Vec<_>> = units
        .iter()
        .map(|unit| {
            let view = match unit {
                CompiledUnit::Module(module) => BinaryIndexedView::Module(&module.module),
                CompiledUnit::Script(script) => BinaryIndexedView::Script(&script.script),
            };
            Disassembler::from_view(view, location::Loc::new(FileHash::empty(), 0, 0))
                .and_then(|d| d.disassemble())
        })
        .collect();
    Ok(disassembled_units?.concat())
}

/// Run the bytecode verifier on the given compiled units and add any diagnostics to the global env.
pub fn run_bytecode_verifier(units: &[AnnotatedCompiledUnit], env: &mut GlobalEnv) -> bool {
    let diags = verify_units(units);
    if !diags.is_empty() {
        add_move_lang_diagnostics(env, diags);
        false
    } else {
        true
    }
}

/// Report any diags in the env to the writer and fail if there are errors.
pub fn check_errors<W>(
    env: &GlobalEnv,
    error_writer: &mut W,
    msg: &'static str,
) -> anyhow::Result<()>
where
    W: WriteColor + Write,
{
    let options = env.get_extension::<Options>().unwrap_or_default();
    env.report_diag(error_writer, options.report_severity());
    if env.has_errors() {
        bail!("exiting with {}", msg);
    } else {
        Ok(())
    }
}

/// Annotate the given compiled units.
pub fn annotate_units(units: Vec<CompiledUnit>) -> Vec<AnnotatedCompiledUnit> {
    units
        .into_iter()
        .map(|u| match u {
            CompiledUnit::Module(named_module) => {
                let loc = named_module.source_map.definition_location;
                AnnotatedCompiledUnit::Module(AnnotatedCompiledModule {
                    loc,
                    module_name_loc: loc,
                    address_name: None,
                    named_module,
                    function_infos: UniqueMap::new(),
                })
            },
            CompiledUnit::Script(named_script) => {
                AnnotatedCompiledUnit::Script(AnnotatedCompiledScript {
                    loc: named_script.source_map.definition_location,
                    named_script,
                    function_info: FunctionInfo {
                        spec_info: Default::default(),
                    },
                })
            },
        })
        .collect()
}

/// Computes the `FilesSourceText` from the global environment, which maps IR loc file hashes
/// into files and sources. This value is used for the package system only.
pub fn make_files_source_text(env: &GlobalEnv) -> FilesSourceText {
    let mut result = FilesSourceText::new();
    for fid in env.get_source_file_ids() {
        if let Some(hash) = env.get_file_hash(fid) {
            let file_name = Symbol::from(env.get_file(fid).to_string_lossy().to_string());
            let file_content = env.get_file_source(fid).to_owned();
            result.insert(hash, (file_name, file_content));
        }
    }
    result
}
